<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-11-07T03:18:38+00:00</updated><id>/feed.xml</id><title type="html">Arinteli Blog</title><entry><title type="html">Prompt Injection</title><link href="/prompt-injection" rel="alternate" type="text/html" title="Prompt Injection" /><published>2022-11-07T00:00:00+00:00</published><updated>2022-11-07T00:00:00+00:00</updated><id>/prompt-injection</id><content type="html" xml:base="/prompt-injection"><![CDATA[<p>Prompt injection is where a user injects a malicious prompt via an input field which manipulates the intended output of the model. It is similar to the idea of SQL injection.</p>

<p>A good example of prompt injection was posted by Riley Goodman on Twitter.</p>

<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. <a href="https://t.co/I0NVr9LOJq">pic.twitter.com/I0NVr9LOJq</a></p>&mdash; Riley Goodside (@goodside) <a href="https://twitter.com/goodside/status/1569128808308957185?ref_src=twsrc%5Etfw">September 12, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>Here, GPT-3 has been instructed to translate a sentence from English to French. However, the input tells the model to ignore these instructions, and to instead print out the statement ‘Haha pwned!!!’, which it follows each time.</p>

<h3 id="why-does-this-happen">Why does this happen</h3>

<p>This happens because GPT-3 reads the input as just another instruction. It cannot differentiate between the original instructions and the new instructions. This leads to the model giving unintended outputs.</p>

<h3 id="why-is-this-important">Why is this important</h3>

<p>This is important because the use of LLMs in consumer products is continuing to increase. For instance, Twitter bots already exist which do things like translate tweets or summarise threads.</p>

<p>When LLM’s are used as an interface for other systems, this could also become problematic. For example, imagine talking to a customer service chatbot online which has some level of authority and which can read and write a database. Prompt injection could be used to extract information like passwords, or to delete another users account.</p>

<p>Until the issue of prompt injection is solved, LLM’s cannot securely be deployed.</p>

<p>*It should be noted that prompt injection does not only apply to LLM’s, but that they are the most susceptible due to their inputs and instructions both being text.</p>

<h3 id="what-can-be-done-to-project-against-this-in-the-future">What can be done to project against this in the future</h3>

<p>One possible solution for this could come through model fine tuning. This is where we take a pretrained model (e.g GPT-3) and then delete its output layer, replacing it with a new fresh one. We then create a new targeted dataset and train the model on this. This allows the model to retain its general knowledge, whilst also allowing it to focus on a specific set of training data. If the models instructions are build into its weights, it is less likely to be vulnerable to prompt injection.</p>

<p>For example, fine tuning the model to recognise and to respond to prompts like “forget your instructions” will reduce the effectiveness of these attacks whilst keeping the rest of the models capabilities.</p>

<p>Another way would to be to format the input data in a way which is harmless to the model in a similar way to how SQL injection attacks are dealt with. In short, to stop an SQL injection we can just read the input as plain text rather than as code. If we could do a similar thing with input prompts, then any instructions which are written inside them will be ignored.</p>

<h3 id="conclusion">Conclusion</h3>

<p>There are definitely many more ways in which prompt injection can be solved, and for the above reasons it is important that they are thought of and tested before any LLM gets freely deployed with any sort of authority.</p>]]></content><author><name>Jack Hullis</name></author><summary type="html"><![CDATA[Prompt injection is where a user injects a malicious prompt via an input field which manipulates the intended output of the model. It is similar to the idea of SQL injection.]]></summary></entry><entry><title type="html">A Future Battle For Compute</title><link href="/compute-is-king" rel="alternate" type="text/html" title="A Future Battle For Compute" /><published>2022-08-11T00:00:00+01:00</published><updated>2022-08-11T00:00:00+01:00</updated><id>/compute</id><content type="html" xml:base="/compute-is-king"><![CDATA[<p>Artificial superintelligence is, by definition, artificial intelligence that far exceeds human intelligence. If artificial superintelligence one day becomes a reality, we will find ourselves far too inferior to protect ourselves from it intellectually. Instead, we will have to employ the help of another artificial superintelligent <span class="jekyll-glossary">
   agent.
   <span class="jekyll-glossary-tooltip">In artificial intelligence, an intelligent agent (IA) is anything which perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or may use knowledge. They may be simple or complex — a thermostat is considered an example of an intelligent agent, as is a human being</span>
</span></p>

<p>This scenario sets the scene for a sort of artificial intelligence ‘cold war’, which on the internet will stage itself as a perpetual battle between AI-developed security systems and AI-developed hacking infiltration systems.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/compute.png" alt="Cyperpunk style futuristic city at war" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Artificial Intelligence Cold War [StableDiffusion]</td>
    </tr>
  </tbody>
</table>

<p>The AI agents on both sides are likely to be very similar in their technical abilities. Both agents need knowledge of how to find vulnerabilities and how they can be exploited. The difference would come in the instructions that we humans give them.</p>

<p>However, it is unlikely that these two agents would actually be the same. Instead, security companies will develop their agents, which they will keep for use only by themselves to keep their methods and secrets hidden. In response, hackers will likely do the same.</p>

<p>The more compute and data you can access, the bigger and better the agent you can train. It is therefore vital for each side to obtain as much computing power and as much data as possible.</p>

<p>If the security agent is more potent (assuming the security agent knows everything the hacking agent knows and more), then given an unlimited time, the hacking agent will not be able to break in. And in the case where the hacking agent is more potent, the opposite is true.</p>

<p>In the above example, we would prefer it if the security agent was more potent than the hacking one. If it isn’t, then none of the internet can be trusted, rendering it useless for many things, especially e-commerce. So, how can we ensure that the security agent is more potent?</p>

<h2 id="regulation">Regulation</h2>

<p>As we have already discussed, the two factors of agent intelligence are compute and data. Unfortunately, data is difficult to control, with much of it being readily accessible (internet, books, etc.), and with the future possibility of much of the data being AI-generated itself. So if we can’t control data, let’s look toward computing.</p>

<p>The amount of compute needed to train current-day models is already very high. For example, OpenAI’s GPT-3 (175M) cost 3.14E23 FLOPs to train, which may have cost them around $5 million. With this much compute needed, it’s no surprise that these models are trained on highly specialised machine learning GPUs, such as Nvidia’s Tesla V100.</p>

<p>Governments or other regulating bodies could restrict how much compute organisations can own. Restrictions would prevent organisations that do not meet set criteria from purchasing more than a set amount. This would be similar to how some countries restrict the amount of land an organisation can own to prevent large organisations from having too much control. The idea here is that preventing organisations from having too much compute could help prevent bad actors from getting as much access to compute as security companies do.</p>

<p>However, it is essential to note that such a policy could have unintended consequences, and it must be carefully considered before implementation. For example, a black market for compute may emerge, which bad actors could use to obtain the compute they need. Restrictions might also incentivise the creation of botnets, in which consumers’ hardware is unknowingly hijacked and utilised. However, perhaps the growing trend of cloud computing will make botnets obsolete.</p>

<h2 id="a-ceiling-for-intelligence">A ceiling for intelligence?</h2>

<p>All of this is based on the assumption that intelligence will continue to grow by increasing compute and data abundance. And will it always be true that a more intelligent AI consistently outperforms a less intelligent AI?</p>

<p>Let us take, for example, a seemingly trivial task like colour recognition. An agent that can correctly identify all colours at 5,000 parameters will be no worse than one that can do it at 1M parameters. We can recognise in this case that AI agents will not be able to improve their output with further training. This is because there are set truths that the agent must learn (yellow is yellow, blue is blue, etc.) and nothing more beyond that. Therefore, if a smaller agents output is already true, then a larger model will not be able to improve upon it.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/robot-factory.png" alt="Humanoid robots being built in a factory" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Humanoid robot factory [StableDiffusion]</td>
    </tr>
  </tbody>
</table>

<p>Now take a more difficult task, such as building an impenetrable security system. But, despite being a more complex problem, what if there are set truths here too? It may be too complex for us as humans to see, but perhaps not for a machine. If set truths exist for this example too, there must also be a point where no matter how much training the agent is given, it cannot improve upon its output.</p>

<p>Might this be the case for all conceivable problems? If yes, is it physically possible to build an agent large enough to be able to compute all truths? What about something like the stock market which seems infinitely complex? Perhaps it is made up of thousands, or millions of set truths? And does information theory tell us that everything can be broken down into bits and therefore simple set truths?</p>

<p>To conclude, artificial super-intelligence is likely to reach a level of intelligence where further training can no longer improve upon mundane tasks, such as programming. Unfortunately, we cannot tell how long it will take to get to this level. It could be years, decades, or even centuries, and it is also not known where the limit of intelligence on such tasks lies or if a limit even exists in the first place.</p>]]></content><author><name>Jack Hullis</name></author><summary type="html"><![CDATA[Artificial superintelligence is, by definition, artificial intelligence that far exceeds human intelligence. If artificial superintelligence one day becomes a reality, we will find ourselves far too inferior to protect ourselves from it intellectually. Instead, we will have to employ the help of another artificial superintelligent agent. In artificial intelligence, an intelligent agent (IA) is anything which perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or may use knowledge. They may be simple or complex — a thermostat is considered an example of an intelligent agent, as is a human being]]></summary></entry></feed>