<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-08-10T01:23:21+01:00</updated><id>/feed.xml</id><entry><title type="html">A Future Battle For Compute</title><link href="/2022/08/02/hello.html" rel="alternate" type="text/html" title="A Future Battle For Compute" /><published>2022-08-02T00:00:00+01:00</published><updated>2022-08-02T00:00:00+01:00</updated><id>/2022/08/02/hello</id><content type="html" xml:base="/2022/08/02/hello.html"><![CDATA[<p>Artificial super intelligence is, by definition, artificial intelligence which far exceeds human intelligence. If artificial super intelligence one day becomes reality, we will find ourselves far too inferior to intellectually defend ourselves from it. Instead, we will have to employ the help of another artificial super intelligent agent.</p>

<p>This scenario sets the scene for an artificial intelligence cold war, which on the internet will stage itself as a constant battle between AI developed security systems and AI developed hacking systems.</p>

<p>The AI’s on both sides are likely to be very similar in their technical abilities. Both agents need knowledge of how to find vulnerabilities and on how they can be exploited. The difference would come in the instructions that they are given by us humans.</p>

<p>In reality however, it is unlikely that these two models would be the same. Security companies will develop their own agents which they will keep for use by only themselves in order to keep their methods and their secrets hidden. In response, hackers will likely do the same.</p>

<p>The more compute and data you have access to, the bigger and better the agent you can train. It is therefore vital for each side to obtain as much computing power and as much data as possible.</p>

<p>If the security agent is stronger (and <span class="jekyll-glossary">
   assuming
   <span class="jekyll-glossary-tooltip">Is it sensible to assume this? Would this only be possible if the two agents were trained on the exact same data? Or does that not matter once the amount of data is bigger than a certain amount.</span>
</span>
 the security agent knows everything the hacking agent knows and more), then given an unlimited time the hacking agent will not be able to break in. And in the case where the hacking agent is stronger, the opposite is true.</p>

<p>In the above example, we would obviously prefer it if the security agent was stronger than the hacking one. If it isn’t, then none of the internet can be trusted, rendering it useless for many things, especially ecommerce. So, how can we ensure that the security agent is stronger?</p>

<h1 id="introduction">Introduction</h1>

<h2 id="the-cat-on-the-matt-went-bat">The Cat On The Matt Went Bat</h2>

<h1 id="regulation">Regulation</h1>

<p>As we have already discussed, the two factors of agent intelligence are compute and data. Data is difficult to control, with much of it being readily accessible (internet, books, etc), and with also the future possibilty of much of the data being AI generated itself. So if we can’t control data, let’s look towards compute.</p>

<p>The amount of compute needed to train current day models is already very high. OpenAI’s GPT-3 (175M) cost 3.14E23 FLOPs to train, which may have cost them around $5 million. With this much compute needed it’s no suprise that these models are trained on highly specialised machine learning GPU’s, such as Nvidia’s Tesla V100.</p>

<ul>
  <li>asi, by definition, will far surpass human intelligence</li>
  <li>in order to defend ourselves against bad AI, we will therefore have to use our own AI systems</li>
  <li>so there will be a constant battle between ai developing more secure systems and ai developing more advanced attacks</li>
</ul>

<p>however,</p>
<ul>
  <li>the more compute you have, the better the AI you can train</li>
  <li>therefore whoever has access to the most compute (and to the most data) will be able to build the better model</li>
  <li>if the security ai is stronger, the hacking ai will not be able to succeed</li>
  <li>if the hacking ai is stronger, then the security ai will not be able to protect<br />
.</li>
  <li>in order to stop the second scenario, restrictions could be placed on how much compute organisations are allowed to own</li>
  <li>not everyone will be able to purchase super-computers - there would be heavy regulation</li>
  <li>in order to be elegible, companies must continuously showcase what they are doing</li>
  <li>this will not affect consumers, as restrictions will be placed on computers orders orders of magnitude more powerful than what we need</li>
  <li>besides, cloud computing is likely the future</li>
</ul>

<p>however,</p>
<ul>
  <li>is intelligence infinite?</li>
  <li>will a larger ai model trained on more data always be better than one which is smaller?</li>
  <li>for easy tasks, such as identifying whether or not a picture is yellow or not, beyond a certain point, further training will not make a difference to the outcome</li>
  <li>now take designing a simple webpage. a huge model trained to build webpages will likely not be any less capable in this task than a super huge model.</li>
  <li>if these ideas are scaled up, perhaps it is possible that a security ai can build an inpenetrable system. and no matter how much more training it is given, it cannot improve on that system. the same goes for the hacking ai.</li>
  <li>asi is likely to reach a point where ‘mundane’ tasks, such as programming, cannot be improved upon anymore.</li>
  <li>how long will it take to reach this level? it could be years, decades, or centuries. it is not known where the limit lies, or if there even is a limit.</li>
</ul>]]></content><author><name>Jack Hullis</name></author><summary type="html"><![CDATA[Artificial super intelligence is, by definition, artificial intelligence which far exceeds human intelligence. If artificial super intelligence one day becomes reality, we will find ourselves far too inferior to intellectually defend ourselves from it. Instead, we will have to employ the help of another artificial super intelligent agent.]]></summary></entry></feed>