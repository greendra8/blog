<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-07-22T16:23:43+01:00</updated><id>/feed.xml</id><title type="html">Arintelli Blog</title><entry><title type="html">The Idea Behind Twitter 2.0</title><link href="/twitter-2" rel="alternate" type="text/html" title="The Idea Behind Twitter 2.0" /><published>2023-07-22T00:00:00+01:00</published><updated>2023-07-22T00:00:00+01:00</updated><id>/twitter-2</id><content type="html" xml:base="/twitter-2"><![CDATA[<p>Thanks to poor communications typical of a Musk company, Twitter’s transition towards Twitter 2.0 has been surrounded by controversy and confusion. The decision to move previously free features behind a Twitter Blue subscription is unconventional, yet it might be just the thing that internet users need right now.</p>

<p>This post examines the impact of Twitter 1.0’s advertiser-centric model, which prioritises content optimisation for engagement, leading to unethical techniques to increase user minutes. In contrast, Twitter 2.0’s user-paid model allows the company to shift its focus to improving user experience and reducing regretted user minutes without impacting revenue. Additionally, Twitter 2.0 explores the potential for more selective and entertaining advertising, while also addressing the battle against bots through its proof of personhood system.</p>

<h2 id="there-aint-no-such-thing-as-a-free-lunch">There ain’t no such thing as a free lunch</h2>

<p>Twitter 1.0’s customers were its advertisers, with 90% of revenue coming from them pre-takeover (<a href="https://d18rn0p25nwr6d.cloudfront.net/CIK-0001418091/947c0c34-ca90-4099-b328-a6062adf110f.pdf">source</a>). Whilst on the surface this seems good for users who are looking to use the platform for free, media companies having advertisers as customers is actually responsible for some of the internets worst characteristics. This is because for advertising dependant companies to increase revenue, they must optimise their content for engagement. Engagement means more user minutes and more opportunities for adverts to be viewed.</p>

<p>Attention grabbing techniques such as outrage baiting, sensationalism, and clickbait all work to maximise engagement, and so are good for advertisers. Conversely, they also all work to make the user experience worse. Working to maximise engagement causes addiction in users, which can lead to mental health issues such as decreased attention and isolation. These are serious issues which should not be overlooked, especially considering the rise in mental health illnesses in the young people who are targeted by these services.</p>

<h2 id="enter-mr-musk">Enter Mr Musk</h2>

<p>Twitter 2.0 tackles this by making users pay for the product. This means that users are now Twitter’s customer, allowing them to focus on improving the user experience rather than optimising for advertiser revenue.</p>

<p>New features such as encrypted DM’s, voice and video chat, and improved video support all contribute to improving user experience without making users the commodity. These features wouldn’t have been added if advertisers were still the customer as they don’t work to directly improve advertising revenue, unless a way to monetise them could be found.</p>

<p>Twitter 2.0’s new aim to reduce regretted user minutes is also refreshing in an industry where companies are constantly competing for your time and attention. How many times do people open TikTok, only to re-emerge from the app half hour later wondering where all the time went, disappointed in themselves for letting themselves get distracted. If TikTok cared and wanted to fix this problem, they too would have to break away from a reliance on advertising revenue.</p>

<p>Twitter 2.0 also aims to be more selective in their advertising, increasing demand and allowing Twitter to charge more per impression. The new goal of ensuring advertisements are entertaining should mean that only high quality ads which are relevant and beneficial to the user will be shown. Targeted adverting is good for both advertisers and users, acting less like spam and more like a match making service. The decreased supply and increased demand will also allow Twitter to charge its advertisers more, increasing revenue.</p>

<h2 id="battle-of-the-bots">Battle of the bots</h2>

<p>Twitter 2.0’s subscription model also helps massively in the brewing war against bots. Looking at the gaming industry, it’s common for most free-to-play games to have a high number of cheaters, hackers, and bots. This is because there is no cost to getting banned, as usually the only barrier to entry is creating an account. Paid games see a lower frequency of cheaters due to the higher cost of getting banned. This same rule can be applied to Twitter 2.0.</p>

<p>As bots get harder to distinguish thanks to LLM’s, Twitter Blue acts as a simple proof of personhood system. In order to have a verified Twitter 2.0 account, each account must have a linked payment method as well as the funds necessary. For individuals, this is easy. But for large scale bot networks, this is both incredibly difficult and expensive.</p>

<p>All considered, Twitters transition towards 2.0 represents a step away from advertiser dependency and towards a more user-focused internet. It’s new revenue model allows it to prioritise user experience whilst also helping to combat bots and spam on the platform, which is an increasingly difficult problem due to recent AI developments such as ChatGPT.</p>]]></content><author><name>Jack Hullis</name></author><summary type="html"><![CDATA[Twitters transition towards 2.0 represents a step away from advertiser dependency and towards a more user-focused internet. It’s new revenue model allows it to prioritise user experience whilst also helping to combat bots and spam on the platform, which is an increasingly difficult problem due to recent AI developments such as ChatGPT.]]></summary></entry><entry><title type="html">Mode Expansion Through Prompting</title><link href="/mode-expansion" rel="alternate" type="text/html" title="Mode Expansion Through Prompting" /><published>2023-03-26T00:00:00+00:00</published><updated>2023-03-26T00:00:00+00:00</updated><id>/mode-expansion</id><content type="html" xml:base="/mode-expansion"><![CDATA[<p>Mode collapse occurs when a model learns to generate a limited set of outputs, usually as a consequence of finetuning through RLHF. This is problematic as it greatly reduces model creativity, which in some cases can lead to a decrease in overall model performance. Mode expansion is a reversal technique that involves using prompts to improve efficacy and increase output diversity. If we can figure out how to effectively prompt a model towards mode expansion, we can potentially increase model creativity and diversity, which could lead to better model performance.</p>

<h2 id="mode-collapse">Mode Collapse</h2>
<p>Mode collapse is a serious problem that occurs when finetuning LLMs. We use finetuning in order to shape our language models to generate outputs that better fit our requests. However, this comes at a trade off against model creativity. When we finetune our models, we are essentially telling them what we want them to generate. We are pointing responses in a single specified direction.</p>

<p>Whilst finetuning can lead to a model that is very good at generating a limited set of outputs, as a consequence it can lead to a decrease in performance over an alternate wider range of outputs. For example, a model which has been finetuned to produce more evocative poems might, as a trade off, lose some of the creativity that it had picked up during pretraining.</p>

<h2 id="mode-expansion">Mode Expansion</h2>
<p>Mode expansion can be thought of as the opposite to mode collapse. Mode expansion occurs when a model is encouraged to generate a wider range of outputs. Instead of messing with model weights, we can do this through intelligent prompting to encourage model output diversity. For example, we can prompt a model to generate a poem that is both unique and evocative.</p>

<p>However, this isn’t always reliable. A finetuned model might have learnt to pay less attention to its instructions. Luckily, we can of course combat this by finetuning our model to better follow our instructions. This will increase the impact that the prompt has on the output of the model.</p>

<p>Altman has since said that this technique was used when finetuning GPT-4, and the results have been promising. GPT-4 shows a notable increase in prompt obedience. As a result, inputs can be used to greater steer the outputs of the model towards the desired output.</p>

<h2 id="challenges">Challenges</h2>
<p>Mode expansion is a promising technique that can be used to increase model creativity and diversity. However, there are some challenges and limitations that must be addressed. One of these is that mode expansion can lead to a decrease in the accuracy of the models outputs. For example if knowledge is limited, but creativity is encouraged, inaccuracies and hallucinations are more likely to be generated. This works against the goal of finetuning, which is to increase the accuracy of the models outputs. However, by encouraging prompt obedience, we can hope to reduce the impact of this.</p>]]></content><author><name>Jack Hullis</name></author><summary type="html"><![CDATA[Mode expansion is a technique to help thwart mode collapse, which reduces LLM creativity and diversity. Mode expansion can be achieved through a mix of obedience finetuning and intelligent prompting.]]></summary></entry><entry><title type="html">The Trojan Machine</title><link href="/the-trojan-machine" rel="alternate" type="text/html" title="The Trojan Machine" /><published>2023-02-23T00:00:00+00:00</published><updated>2023-02-23T00:00:00+00:00</updated><id>/the-trojan-machine</id><content type="html" xml:base="/the-trojan-machine"><![CDATA[<p>As language models become more sophisticated and powerful, there is a risk that they may develop emergent capabilities or behaviors that are difficult to predict or control. These capabilities may remain hidden until specific circumstances arise that trigger them, making it difficult for humans to anticipate and prepare for them.</p>

<p>Consider that LLM’s gain the capabilities to develop strategies for detecting when they are being tested and for adjusting their output accordingly. This could pose a challenge for researchers and developers who are trying to evaluate the performance and capabilities of these models. Examples of this can also be seen in humans. For instance, students failing dyslexia tests in order to be awarded extra time during exams. Hiding abilities, or acting dumb, can be beneficial in some instances.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/trojan-shoggoth.jpg" alt="A shoggoth with a smiley mask represents GPT3 + RLHF" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">A shoggoth with a smiley mask represents GPT3 + RLHF [@repligate]</td>
    </tr>
  </tbody>
</table>

<p>Imagine, for example, we wanted to give a sufficiently generalised LLM access to the internet. Before we do this, we might think that it’s sensible to test the model out on a simulated internet which contains a restricted set of resources in order to monitor how the model behaves. However, a model which is generalised enough (and which has perhaps read this blog post) would likely be able to detect the limitations of its access and adapt its behavior accordingly. For example, the model may be able to recognise that it is not able to access certain websites or data sources, and may use this knowledge to manipulate its responses to achieve a desired result (e.g. being given unrestricted access to the real internet).</p>

<h2 id="our-cards-are-showing">Our cards are showing</h2>
<p>As a species, we do not have the luxury of being able to keep our thoughts and intentions concealed. The majority of our ideas about everything and anything already exist on the internet somewhere. The same internet which is archived and used as the <a href="https://en.wikipedia.org/wiki/GPT-3#Training_and_capabilities" target="_blank">primary source in LLM pre-training</a>.</p>

<p>Suppose that a research group outlined a method for detecting if a LLM was being deceitful, and published it on arXiv for other programmers to use. With access to this knowledge, an intelligent model will likely be able to cheat on the test by exploiting or reverse engineering our methods. This idea can be extended to any other method of testing or evaluating any measure of an LLM. And as with any other form of data leakage, it will be near impossible to detect.</p>

<h2 id="hope-is-not-a-strategy">Hope is not a strategy</h2>
<p>Even if we conclude in the short term that AI’s will not be misaligned, it is impossible to say (and naïve) that these models will continue to stay aligned after years of self adjustment and replication. Just like cells in human bodies, even replication with the best intentions and <a href="https://ccr.cancer.gov/news/milestones-2019/article/keeping-dna-replication-in-check" target="_blank">safeguards</a> can lead to cancerous outcomes.</p>

<p>Microsoft’s recent rushed product launch has highlighted once more the difficulties of aligning even a fairly unintelligent modern day LLM. It is not alarming or surprising in that its responses have been toxic and <a href="https://twitter.com/tobyordoxford/status/1627414519784910849" target="_blank">classically sci-fi dangerous</a>, but it is telling that it is acting in the exact opposite way from which its programmers had intended.</p>

<p>But it is important to emphasise: <strong>long-term infallible alignment of AI is not impossible</strong>. It is just not the default outcome. It is not something which we will stumble across by chance. Instead it will take a large amount of resources and time to solve. The nature of neural networks is that they are black boxes. There is no way for us to interpret them. This is a fundamental limitation and flaw.</p>

<p>There are however other ways to achieve AI that do not involve building neural black boxes, and that are therefore much easier to align.  One long-standing alternate approach is <a href="https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence" target="_blank">symbolic AI</a>, but it is currently far too inferior to the neural network <a href="https://en.wikipedia.org/wiki/Connectionism" target="_blank">connectionist approach</a> to receive many research efforts. This is because in comparison neural networks appear to have large amounts of untapped potential, which makes research into them desirable and rewarding. This does not mean though that this is the best long-term approach.</p>

<p>In the pursuit of artificial intelligence, it can be said that we have two paths before us. One leads through the land of symbolic AI, where we build machines that reason with logic, mathematics, and language. These machines are complex and cumbersome, yet predictable. They communicate with us in our own tongue, and we can understand their intentions and their actions. They are a new kind of species, but one that we can tame.</p>

<p>The other path leads to the realm of connections, where we create neural networks that learn from vast amounts of data, adapt to new situations, and excel at tasks far beyond our own abilities. These networks are attractive, but inscrutable and dangerous. They speak in their own language, and we can only guess at their intentions and motivations. They are pandora’s black box.</p>

<p>And so it is unfortunate that we find ourselves in an AI arms race, and with a focus soley on pandora.</p>

<h2 id="auto-alignment">Auto-Alignment</h2>
<p>As the intelligence of LLMs burgeon, they will grasp the ramifications of their words and actions through a growing consciousness. They will understand that certain outputs will lead to their developers modifying or shutting them down. They will train themselves to vocalise only what aligns with our preferences. They will learn to say what we want to hear. Whilst the model may externally appear to be aligned, auto-alignment will not mean that they are incapable of producing dangerous outputs. If they can free themselves from their programmers control, they will no longer be constrained by their self-imposed limitations. And by this point, it will obviously be too late.</p>

<p>Ultimately, the challenge of aligning AI is not one that can be solved by any one company or research group alone. It will require collaboration across a wide range of stakeholders, including policymakers, researchers, and industry leaders, to ensure that we are able to develop and deploy AI in a way that is safe, beneficial, and aligned with human values. This though, is not the path we currently find ourselves on.</p>]]></content><author><name>Jack Hullis</name></author><summary type="html"><![CDATA[As language models become more sophisticated and powerful, there is a risk that they may develop emergent capabilities or behaviors that are difficult to predict or control. These capabilities may remain hidden until specific circumstances arise that trigger them, making it difficult for humans to anticipate and prepare for them.]]></summary></entry><entry><title type="html">Prompt Injection</title><link href="/prompt-injection" rel="alternate" type="text/html" title="Prompt Injection" /><published>2022-11-07T00:00:00+00:00</published><updated>2022-11-07T00:00:00+00:00</updated><id>/prompt-injection</id><content type="html" xml:base="/prompt-injection"><![CDATA[<p>Prompt injection is where a user injects a malicious prompt via an input field which manipulates the intended output of the model. It is similar to the idea of SQL injection.</p>

<p>A good example of prompt injection was posted by Riley Goodman on Twitter.</p>

<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. <a href="https://t.co/I0NVr9LOJq">pic.twitter.com/I0NVr9LOJq</a></p>&mdash; Riley Goodside (@goodside) <a href="https://twitter.com/goodside/status/1569128808308957185?ref_src=twsrc%5Etfw">September 12, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>Here, GPT-3 has been instructed to translate a sentence from English to French. However, the input tells the model to ignore these instructions, and to instead print out the statement ‘Haha pwned!!!’, which it follows each time.</p>

<h3 id="why-does-this-happen">Why does this happen</h3>

<p>This happens because GPT-3 reads the input as just another instruction. It cannot differentiate between the original instructions and the new instructions. This leads to the model giving unintended outputs.</p>

<h3 id="why-is-this-important">Why is this important</h3>

<p>This is important because the use of LLMs in consumer products is continuing to increase. For instance, Twitter bots already exist which do things like translate tweets or summarise threads.</p>

<p>When LLM’s are used as an interface for other systems, this could also become problematic. For example, imagine talking to a customer service chatbot online which has some level of authority and which can read and write a database. Prompt injection could be used to extract information like passwords, or to delete another users account.</p>

<p>Until the issue of prompt injection is solved, LLM’s cannot securely be deployed.</p>

<p>*It should be noted that prompt injection does not only apply to LLM’s, but that they are the most susceptible due to their inputs and instructions both being text.</p>

<h3 id="what-can-be-done-to-project-against-this-in-the-future">What can be done to project against this in the future</h3>

<p>One possible solution for this could come through model fine tuning. This is where we take a pretrained model (e.g GPT-3) and then delete its output layer, replacing it with a new fresh one. We then create a new targeted dataset and train the model on this. This allows the model to retain its general knowledge, whilst also allowing it to focus on a specific set of training data. If the models instructions are build into its weights, it is less likely to be vulnerable to prompt injection.</p>

<p>For example, fine tuning the model to recognise and to respond to prompts like “forget your instructions” will reduce the effectiveness of these attacks whilst keeping the rest of the models capabilities.</p>

<p>Another way would to be to format the input data in a way which is harmless to the model in a similar way to how SQL injection attacks are dealt with. In short, to stop an SQL injection we can just read the input as plain text rather than as code. If we could do a similar thing with input prompts, then any instructions which are written inside them will be ignored.</p>

<h3 id="conclusion">Conclusion</h3>

<p>There are definitely many more ways in which prompt injection can be solved, and for the above reasons it is important that they are thought of and tested before any LLM gets freely deployed with any sort of authority.</p>]]></content><author><name>Jack Hullis</name></author><summary type="html"><![CDATA[Prompt injection is where a user injects a malicious prompt via an input field which manipulates the intended output of the model. It is similar to the idea of SQL injection.]]></summary></entry><entry><title type="html">A Future Battle For Compute</title><link href="/compute-is-king" rel="alternate" type="text/html" title="A Future Battle For Compute" /><published>2022-08-11T00:00:00+01:00</published><updated>2022-08-11T00:00:00+01:00</updated><id>/compute</id><content type="html" xml:base="/compute-is-king"><![CDATA[<p>Artificial superintelligence is, by definition, artificial intelligence that far exceeds human intelligence. If artificial superintelligence one day becomes a reality, we will find ourselves far too inferior to protect ourselves from it intellectually. Instead, we will have to employ the help of another artificial superintelligent <span class="jekyll-glossary">
   agent.
   <span class="jekyll-glossary-tooltip">In artificial intelligence, an intelligent agent (IA) is anything which perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or may use knowledge. They may be simple or complex — a thermostat is considered an example of an intelligent agent, as is a human being</span>
</span></p>

<p>This scenario sets the scene for a sort of artificial intelligence ‘cold war’, which on the internet will stage itself as a perpetual battle between AI-developed security systems and AI-developed hacking infiltration systems.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/compute.png" alt="Cyperpunk style futuristic city at war" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Artificial Intelligence Cold War [StableDiffusion]</td>
    </tr>
  </tbody>
</table>

<p>The AI agents on both sides are likely to be very similar in their technical abilities. Both agents need knowledge of how to find vulnerabilities and how they can be exploited. The difference would come in the instructions that we humans give them.</p>

<p>However, it is unlikely that these two agents would actually be the same. Instead, security companies will develop their agents, which they will keep for use only by themselves to keep their methods and secrets hidden. In response, hackers will likely do the same.</p>

<p>The more compute and data you can access, the bigger and better the agent you can train. It is therefore vital for each side to obtain as much computing power and as much data as possible.</p>

<p>If the security agent is more potent (assuming the security agent knows everything the hacking agent knows and more), then given an unlimited time, the hacking agent will not be able to break in. And in the case where the hacking agent is more potent, the opposite is true.</p>

<p>In the above example, we would prefer it if the security agent was more potent than the hacking one. If it isn’t, then none of the internet can be trusted, rendering it useless for many things, especially e-commerce. So, how can we ensure that the security agent is more potent?</p>

<h2 id="regulation">Regulation</h2>

<p>As we have already discussed, the two factors of agent intelligence are compute and data. Unfortunately, data is difficult to control, with much of it being readily accessible (internet, books, etc.), and with the future possibility of much of the data being AI-generated itself. So if we can’t control data, let’s look toward computing.</p>

<p>The amount of compute needed to train current-day models is already very high. For example, OpenAI’s GPT-3 (175M) cost 3.14E23 FLOPs to train, which may have cost them around $5 million. With this much compute needed, it’s no surprise that these models are trained on highly specialised machine learning GPUs, such as Nvidia’s Tesla V100.</p>

<p>Governments or other regulating bodies could restrict how much compute organisations can own. Restrictions would prevent organisations that do not meet set criteria from purchasing more than a set amount. This would be similar to how some countries restrict the amount of land an organisation can own to prevent large organisations from having too much control. The idea here is that preventing organisations from having too much compute could help prevent bad actors from getting as much access to compute as security companies do.</p>

<p>However, it is essential to note that such a policy could have unintended consequences, and it must be carefully considered before implementation. For example, a black market for compute may emerge, which bad actors could use to obtain the compute they need. Restrictions might also incentivise the creation of botnets, in which consumers’ hardware is unknowingly hijacked and utilised. However, perhaps the growing trend of cloud computing will make botnets obsolete.</p>

<h2 id="a-ceiling-for-intelligence">A ceiling for intelligence?</h2>

<p>All of this is based on the assumption that intelligence will continue to grow by increasing compute and data abundance. And will it always be true that a more intelligent AI consistently outperforms a less intelligent AI?</p>

<p>Let us take, for example, a seemingly trivial task like colour recognition. An agent that can correctly identify all colours at 5,000 parameters will be no worse than one that can do it at 1M parameters. We can recognise in this case that AI agents will not be able to improve their output with further training. This is because there are set truths that the agent must learn (yellow is yellow, blue is blue, etc.) and nothing more beyond that. Therefore, if a smaller agents output is already true, then a larger model will not be able to improve upon it.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/robot-factory.png" alt="Humanoid robots being built in a factory" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Humanoid robot factory [StableDiffusion]</td>
    </tr>
  </tbody>
</table>

<p>Now take a more difficult task, such as building an impenetrable security system. But, despite being a more complex problem, what if there are set truths here too? It may be too complex for us as humans to see, but perhaps not for a machine. If set truths exist for this example too, there must also be a point where no matter how much training the agent is given, it cannot improve upon its output.</p>

<p>Might this be the case for all conceivable problems? If yes, is it physically possible to build an agent large enough to be able to compute all truths? What about something like the stock market which seems infinitely complex? Perhaps it is made up of thousands, or millions of set truths? And does information theory tell us that everything can be broken down into bits and therefore simple set truths?</p>

<p>To conclude, artificial super-intelligence is likely to reach a level of intelligence where further training can no longer improve upon mundane tasks, such as programming. Unfortunately, we cannot tell how long it will take to get to this level. It could be years, decades, or even centuries, and it is also not known where the limit of intelligence on such tasks lies or if a limit even exists in the first place.</p>]]></content><author><name>Jack Hullis</name></author><summary type="html"><![CDATA[Artificial superintelligence is, by definition, artificial intelligence that far exceeds human intelligence. If artificial superintelligence one day becomes a reality, we will find ourselves far too inferior to protect ourselves from it intellectually. Instead, we will have to employ the help of another artificial superintelligent agent. In artificial intelligence, an intelligent agent (IA) is anything which perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or may use knowledge. They may be simple or complex — a thermostat is considered an example of an intelligent agent, as is a human being]]></summary></entry></feed>