<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Prompt Injection | Arintelli Blog</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Prompt Injection" />
<meta name="author" content="Jack Hullis" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Prompt injection is where a user injects a malicious prompt via an input field which manipulates the intended output of the model. It is similar to the idea of SQL injection." />
<meta property="og:description" content="Prompt injection is where a user injects a malicious prompt via an input field which manipulates the intended output of the model. It is similar to the idea of SQL injection." />
<meta property="og:site_name" content="Arintelli Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-07T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Prompt Injection" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jack Hullis"},"dateModified":"2022-11-07T00:00:00+00:00","datePublished":"2022-11-07T00:00:00+00:00","description":"Prompt injection is where a user injects a malicious prompt via an input field which manipulates the intended output of the model. It is similar to the idea of SQL injection.","headline":"Prompt Injection","mainEntityOfPage":{"@type":"WebPage","@id":"/prompt-injection"},"url":"/prompt-injection"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Arintelli Blog" />
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y838MPYL1L"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-Y838MPYL1L');
    </script>
    
  </head>
  <header>
    <nav>
  <a class="logo" href="/">Jack Hullis' Blog</a>
      
        <li class="menu-item"><a href="/" >Home</a></li>
      
        <li class="menu-item"><a href="/blog" >Blog</a></li>
      
        <li class="menu-item"><a href="/about" >About</a></li>
      
      <a class="twitter" href="https://twitter.com/jackhullis"/ target="_blank"><i class="fa fa-twitter"></i>@jackhullis</a>
  </nav>
  </header>
  <body contenteditable="false">
    <div class="content">
      <h1>Prompt Injection</h1>
<p id="date-author">07 Nov 2022 - Jack Hullis</p>

<div class="sidebar">
    <ul id="toc" class="toc__list">
        <li class="toc-entry toc-h2"><a href="#">Prompt Injection</a></li>
    </ul>
    <ul id="toc" class="toc__list">
<li class="toc-entry toc-h3"><a href="#why-does-this-happen">Why does this happen</a></li>
<li class="toc-entry toc-h3"><a href="#why-is-this-important">Why is this important</a></li>
<li class="toc-entry toc-h3"><a href="#what-can-be-done-to-project-against-this-in-the-future">What can be done to project against this in the future</a></li>
<li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li>
</ul>
    </div>


<p>Prompt injection is where a user injects a malicious prompt via an input field which manipulates the intended output of the model. It is similar to the idea of SQL injection.</p>

<p>A good example of prompt injection was posted by Riley Goodman on Twitter.</p>

<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. <a href="https://t.co/I0NVr9LOJq">pic.twitter.com/I0NVr9LOJq</a></p>&mdash; Riley Goodside (@goodside) <a href="https://twitter.com/goodside/status/1569128808308957185?ref_src=twsrc%5Etfw">September 12, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>Here, GPT-3 has been instructed to translate a sentence from English to French. However, the input tells the model to ignore these instructions, and to instead print out the statement ‘Haha pwned!!!’, which it follows each time.</p>

<h3 id="why-does-this-happen">Why does this happen</h3>

<p>This happens because GPT-3 reads the input as just another instruction. It cannot differentiate between the original instructions and the new instructions. This leads to the model giving unintended outputs.</p>

<h3 id="why-is-this-important">Why is this important</h3>

<p>This is important because the use of LLMs in consumer products is continuing to increase. For instance, Twitter bots already exist which do things like translate tweets or summarise threads.</p>

<p>When LLM’s are used as an interface for other systems, this could also become problematic. For example, imagine talking to a customer service chatbot online which has some level of authority and which can read and write a database. Prompt injection could be used to extract information like passwords, or to delete another users account.</p>

<p>Until the issue of prompt injection is solved, LLM’s cannot securely be deployed.</p>

<p>*It should be noted that prompt injection does not only apply to LLM’s, but that they are the most susceptible due to their inputs and instructions both being text.</p>

<h3 id="what-can-be-done-to-project-against-this-in-the-future">What can be done to project against this in the future</h3>

<p>One possible solution for this could come through model fine tuning. This is where we take a pretrained model (e.g GPT-3) and then delete its output layer, replacing it with a new fresh one. We then create a new targeted dataset and train the model on this. This allows the model to retain its general knowledge, whilst also allowing it to focus on a specific set of training data. If the models instructions are build into its weights, it is less likely to be vulnerable to prompt injection.</p>

<p>For example, fine tuning the model to recognise and to respond to prompts like “forget your instructions” will reduce the effectiveness of these attacks whilst keeping the rest of the models capabilities.</p>

<p>Another way would to be to format the input data in a way which is harmless to the model in a similar way to how SQL injection attacks are dealt with. In short, to stop an SQL injection we can just read the input as plain text rather than as code. If we could do a similar thing with input prompts, then any instructions which are written inside them will be ignored.</p>

<h3 id="conclusion">Conclusion</h3>

<p>There are definitely many more ways in which prompt injection can be solved, and for the above reasons it is important that they are thought of and tested before any LLM gets freely deployed with any sort of authority.</p>




    </div>
    <hr>
    <footer>Powered by HTML and Jekyll</footer>
  </body>
</html>

